{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMKtr7hXZliftP6ng2yKGu6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5j3QH_PeOi50"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import string\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "bNl2UoTKR9tD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('mar.txt',encoding='utf-8', sep='\t',  names=['English', 'Hindi', 'Attribution'])"
      ],
      "metadata": {
        "id": "yRb8ivQ1XJBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "zq6-QXinXLGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(['Attribution'], axis=1, inplace=True)\n"
      ],
      "metadata": {
        "id": "ti_hoiQZXOk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()\n"
      ],
      "metadata": {
        "id": "uNVJHTQfXQ7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "haCvDmPfXTQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.English = df.English.apply(lambda x: \" \".join(x.split()))\n",
        "df.Hindi = df.Hindi.apply(lambda x: \" \".join(x.split()))\n"
      ],
      "metadata": {
        "id": "2MLytBZhXVRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.English = df.English.apply(lambda x: x.lower())\n"
      ],
      "metadata": {
        "id": "qwVir6v6XXXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"contraction_expansion.txt\", 'rb') as fp:\n",
        "    contractions= pickle.load(fp)"
      ],
      "metadata": {
        "id": "roA6AvziXarw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def expand_contras(text):\n",
        "    if type(text) is str:\n",
        "        for key in contractions:\n",
        "            value = contractions[key]\n",
        "            text = text.replace(key, value)\n",
        "        return text\n",
        "    else:\n",
        "        return text\n"
      ],
      "metadata": {
        "id": "-izptFRBXgT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.sample(10)\n"
      ],
      "metadata": {
        "id": "EaQOQ2O5XiX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xyz = \"i'm don't he'll you'll\"\n",
        "expand_contras(xyz)\n"
      ],
      "metadata": {
        "id": "COBipoUwXkDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.English = df.English.apply(lambda x: expand_contras(x))\n"
      ],
      "metadata": {
        "id": "UEsqKEalXmLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "hhAn_wJGBxWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.sample(5)\n"
      ],
      "metadata": {
        "id": "Fs3LD2nPXoTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator= str.maketrans('','', string.punctuation)\n"
      ],
      "metadata": {
        "id": "Or-TVdHkXp87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.English= df.English.apply(lambda x: x.translate(translator))\n",
        "df.Hindi= df.Hindi.apply(lambda x: x.translate(translator))\n"
      ],
      "metadata": {
        "id": "FfkssW4bXrQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.sample(5)\n"
      ],
      "metadata": {
        "id": "TVETEnn4XuHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n"
      ],
      "metadata": {
        "id": "9LypB8GSXvx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.English= df.English.apply(lambda x: re.sub(r'[\\d]+','', x))\n",
        "df.Hindi= df.Hindi.apply(lambda x: re.sub(r'[\\d]+','', x))\n"
      ],
      "metadata": {
        "id": "FlmyH7iQXxTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['en_word_count']= df.English.apply(lambda x: len(x.split()))\n",
        "df['mar_word_count']= df.Hindi.apply(lambda x: len(x.split()))\n"
      ],
      "metadata": {
        "id": "j9d1HweoXzAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['mar_char_count']= df.Hindi.apply(lambda x: len(\"\".join(x.split())))\n",
        "df['en_char_count']= df.English.apply(lambda x: len(\"\".join(x.split())))\n"
      ],
      "metadata": {
        "id": "t3XesJyEX1LN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()\n"
      ],
      "metadata": {
        "id": "eafusX_WX4w_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,10))\n",
        "sns.kdeplot(x=df.en_word_count, shade=True, color='blue', label='Real')\n"
      ],
      "metadata": {
        "id": "x7H7qZJeX6DY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max(df.en_word_count)\n"
      ],
      "metadata": {
        "id": "T3PomEeNX7ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,10))\n",
        "sns.kdeplot(x=df.mar_word_count, shade=True, color='green', label='Real')\n"
      ],
      "metadata": {
        "id": "4d7aq2NjX9w9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max(df.mar_word_count)\n"
      ],
      "metadata": {
        "id": "-AEj9-DiX_3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "sns.distplot(x=df.en_char_count)\n"
      ],
      "metadata": {
        "id": "ctWMyp9QYBn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "sns.distplot(x=df.mar_char_count)\n"
      ],
      "metadata": {
        "id": "9otPvfkYYC3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_word_cloud(data):\n",
        "    words=\"\"\n",
        "    for sent in data:\n",
        "        sent= str(sent)\n",
        "        sent=sent.lower()\n",
        "        tokens= sent.split()\n",
        "        words +=\" \".join(tokens)+\" \"\n",
        "    plt.figure(figsize=(1,12))\n",
        "    wordcloud= WordCloud(width=800,height=800, background_color='aqua').generate(words)\n",
        "    plt.imshow(wordcloud)\n",
        "    plt.axis('off')\n"
      ],
      "metadata": {
        "id": "mx3kozx0YEtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_word_cloud(df.English)\n"
      ],
      "metadata": {
        "id": "GHwvmQ1RYGdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()\n"
      ],
      "metadata": {
        "id": "yak_3ePLYH74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"cleaned.csv\",index=None)\n"
      ],
      "metadata": {
        "id": "1e2iYGmNYKiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import os\n"
      ],
      "metadata": {
        "id": "pEj1DgywYMQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"cleaned.csv\")"
      ],
      "metadata": {
        "id": "jBG2KTrSY0jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()\n"
      ],
      "metadata": {
        "id": "oZl6jyF0Y5gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Hindi'] =df.Hindi.apply(lambda x: 'sos '+ x + ' eos')\n"
      ],
      "metadata": {
        "id": "Y0ehkjR0Y6zr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()\n"
      ],
      "metadata": {
        "id": "EFn4e8LzZMVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_texts = df.English.to_list()\n",
        "mar_texts = df.Hindi.to_list()\n"
      ],
      "metadata": {
        "id": "zyeO5alSZNVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n"
      ],
      "metadata": {
        "id": "oXyDGZYGZPN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_sent(text):\n",
        "  '''\n",
        "  Take list on texts as input and\n",
        "  returns its tokenizer and enocded text\n",
        "  '''\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(text)\n",
        "\n",
        "  return tokenizer, tokenizer.texts_to_sequences(text)\n"
      ],
      "metadata": {
        "id": "GxqQ6BtWZQQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_tokenizer, eng_encoded= tokenize_sent(text= eng_texts)\n",
        "mar_tokenizer, mar_encoded= tokenize_sent(text= mar_texts)\n"
      ],
      "metadata": {
        "id": "drp7Ps7iZTXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_encoded[100:105]\n"
      ],
      "metadata": {
        "id": "bgTsxZPiZVQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_index_word = eng_tokenizer.index_word\n",
        "eng_word_indec= eng_tokenizer.word_index\n"
      ],
      "metadata": {
        "id": "34AtrpP1ZW1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ENG_VOCAB_SIZE = len(eng_tokenizer.word_counts)+1\n",
        "ENG_VOCAB_SIZE\n"
      ],
      "metadata": {
        "id": "CxK7QyckZYae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mar_encoded[3000:3005]\n"
      ],
      "metadata": {
        "id": "G9kdD0fdZZ69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mar_index_word = mar_tokenizer.index_word\n",
        "mar_word_index= mar_tokenizer.word_index\n"
      ],
      "metadata": {
        "id": "MglEGBNnZbKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAR_VOCAB_SIZE=len(mar_tokenizer.word_counts)+1\n",
        "MAR_VOCAB_SIZE\n"
      ],
      "metadata": {
        "id": "QXe4u4kFZcfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_eng_len = 0\n",
        "for i in range(len(eng_encoded)):\n",
        "  if len(eng_encoded[i]) > max_eng_len:\n",
        "    max_eng_len= len(eng_encoded[i])\n",
        "\n",
        "max_mar_len = 0\n",
        "for i in range(len(mar_encoded)):\n",
        "  if len(eng_encoded[i]) > max_mar_len:\n",
        "    max_mar_len= len(mar_encoded[i])\n"
      ],
      "metadata": {
        "id": "OF_VQS3LZmQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(max_eng_len)\n",
        "max_mar_len\n"
      ],
      "metadata": {
        "id": "UUODxTOkZq1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
      ],
      "metadata": {
        "id": "4qjjlMsZZsO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_padded = pad_sequences(eng_encoded, maxlen=max_eng_len, padding='post')\n",
        "mar_padded = pad_sequences(mar_encoded, maxlen=max_mar_len, padding='post')\n"
      ],
      "metadata": {
        "id": "wQ7ZZ8JZZtno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_padded\n"
      ],
      "metadata": {
        "id": "C5r_BEBPZu6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mar_padded.shape\n"
      ],
      "metadata": {
        "id": "oNfm3j4wZwT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_padded= np.array(eng_padded)\n",
        "mar_padded= np.array(mar_padded)\n"
      ],
      "metadata": {
        "id": "LYwLcj5sZxj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "9gAP_M8eZy3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(eng_padded, mar_padded, test_size=0.1, random_state=0)\n"
      ],
      "metadata": {
        "id": "lX7ib7xFZ0Fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n"
      ],
      "metadata": {
        "id": "YhQe0ReQZ1Sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense, LSTM, Bidirectional, Embedding, Concatenate, Dropout\n",
        "from tensorflow.keras import Input, Model\n"
      ],
      "metadata": {
        "id": "J7BF9NayZ2j3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from BahdanauAttention import AttentionLayer\n"
      ],
      "metadata": {
        "id": "gZ6JGXRFdBnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "class AttentionLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        encoder_output, decoder_output = inputs\n",
        "        # Define your attention mechanism here\n",
        "        # For example:\n",
        "        attention_scores = tf.matmul(decoder_output, encoder_output, transpose_b=True)\n",
        "        attention_weights = tf.nn.softmax(attention_scores, axis=-1)\n",
        "        attention_result = tf.matmul(attention_weights, encoder_output)\n",
        "\n",
        "        return attention_result, attention_weights\n"
      ],
      "metadata": {
        "id": "npmW7guDd6Xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_inputs = Input(shape=(max_eng_len,))\n",
        "enc_emb = Embedding(ENG_VOCAB_SIZE, 1024)(encoder_inputs)\n",
        "\n",
        "# Bidirectional lstm layer\n",
        "enc_lstm1 = Bidirectional(LSTM(256,return_sequences=True,return_state=True))\n",
        "encoder_outputs1, forw_state_h, forw_state_c, back_state_h, back_state_c = enc_lstm1(enc_emb)\n",
        "\n",
        "final_enc_h = Concatenate()([forw_state_h,back_state_h])\n",
        "final_enc_c = Concatenate()([forw_state_c,back_state_c])\n",
        "\n",
        "encoder_states =[final_enc_h, final_enc_c]\n",
        "\n",
        "# Set up the decoder.\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "dec_emb_layer = Embedding(MAR_VOCAB_SIZE, 1024)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "#LSTM using encoder_states as initial state\n",
        "decoder_lstm = LSTM(512, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
        "\n",
        "#Attention Layer\n",
        "attention_layer = AttentionLayer()\n",
        "attention_result, attention_weights = attention_layer([encoder_outputs1, decoder_outputs])\n",
        "\n",
        "# Concat attention output and decoder LSTM output\n",
        "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attention_result])\n",
        "\n",
        "#Dense layer\n",
        "decoder_dense = Dense(MAR_VOCAB_SIZE, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_concat_input)\n",
        "\n",
        "\n",
        "# Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n"
      ],
      "metadata": {
        "id": "iOYWYMf6bw0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()\n"
      ],
      "metadata": {
        "id": "aufBrbSjb0T-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(model,show_shapes=True)\n"
      ],
      "metadata": {
        "id": "sRjtDu9neGti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "mpPjuQNLeI05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"checkpoint.weights.h5\", monitor='val_accuracy', save_weights_only=True)\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy', patience=5)\n",
        "\n",
        "callbacks_list = [checkpoint, early_stopping]\n"
      ],
      "metadata": {
        "id": "Kce0jHDceOaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input_data = X_train\n",
        "decoder_input_data = y_train[:,:-1]\n",
        "decoder_target_data =  y_train[:,1:]\n",
        "\n",
        "# Testing\n",
        "encoder_input_test = X_test\n",
        "decoder_input_test = y_test[:,:-1]\n",
        "decoder_target_test=  y_test[:,1:]\n"
      ],
      "metadata": {
        "id": "SzsBvHMJeQN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  EPOCHS= 50 #@param {type:'slider',min:10,max:100, step:10 }\n"
      ],
      "metadata": {
        "id": "uurU0SNyfL1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "history = model.fit([encoder_input_data, decoder_input_data],decoder_target_data,\n",
        "                    epochs=EPOCHS,\n",
        "                    batch_size=128,\n",
        "                    validation_data = ([encoder_input_test, decoder_input_test],decoder_target_test),\n",
        "                    callbacks= callbacks_list)\n",
        "#model.save_weights(\"model.h5\") # can give whole path to save model"
      ],
      "metadata": {
        "id": "Ap3PCyCMfNoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights(\"model.weights.h5\")"
      ],
      "metadata": {
        "id": "QY9QQWJinmZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(\"model.weights.h5\")"
      ],
      "metadata": {
        "id": "11WnjigynvOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_model = Model(encoder_inputs, outputs = [encoder_outputs1, final_enc_h, final_enc_c])\n",
        "\n",
        "decoder_state_h = Input(shape=(512,))\n",
        "decoder_state_c = Input(shape=(512,))\n",
        "decoder_hidden_state_input = Input(shape=(36,512))\n",
        "\n",
        "dec_states = [decoder_state_h, decoder_state_c]\n",
        "\n",
        "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=dec_states)\n",
        "\n",
        "# Attention inference\n",
        "attention_result_inf, attention_weights_inf = attention_layer([decoder_hidden_state_input, decoder_outputs2])\n",
        "\n",
        "decoder_concat_input_inf = Concatenate(axis=-1, name='concat_layer')([decoder_outputs2, attention_result_inf])\n",
        "\n",
        "dec_states2= [state_h2, state_c2]\n",
        "\n",
        "decoder_outputs2 = decoder_dense(decoder_concat_input_inf)\n",
        "\n",
        "decoder_model= Model(\n",
        "                    [decoder_inputs] + [decoder_hidden_state_input, decoder_state_h, decoder_state_c],\n",
        "                     [decoder_outputs2]+ dec_states2)"
      ],
      "metadata": {
        "id": "ShV351kwn37T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predicted_sentence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    enc_output, enc_h, enc_c = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0] = mar_word_index['sos']\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + [enc_output, enc_h, enc_c ])\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        if sampled_token_index == 0:\n",
        "          break\n",
        "        else:\n",
        "            # convert max index number to marathi word\n",
        "            sampled_char = mar_index_word[sampled_token_index]\n",
        "\n",
        "        if (sampled_char!='end'):\n",
        "            # aapend it ti decoded sent\n",
        "            decoded_sentence += ' '+sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length or find stop token.\n",
        "        if (sampled_char == 'eos' or len(decoded_sentence.split()) >= 36):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update states\n",
        "        enc_h, enc_c = h, c\n",
        "\n",
        "    return decoded_sentence\n"
      ],
      "metadata": {
        "id": "2CXQmN9An6Z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_hindi_sentence(input_sequence):\n",
        "    sentence =''\n",
        "    for i in input_sequence:\n",
        "      if i!=0 :\n",
        "        sentence =sentence +mar_index_word[i]+' '\n",
        "    return sentence\n",
        "\n",
        "def get_english_sentence(input_sequence):\n",
        "    sentence =''\n",
        "    for i in input_sequence:\n",
        "      if i!=0:\n",
        "        sentence =sentence +eng_index_word[i]+' '\n",
        "    return sentence\n"
      ],
      "metadata": {
        "id": "PPne9ON9n9CH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_test)\n"
      ],
      "metadata": {
        "id": "QOHRgxFAn_5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in np.random.randint(10, 50, size=15):\n",
        "    # Get English and Marathi sentences\n",
        "    print(\"English Sentence:\", get_english_sentence(X_test[i]))\n",
        "    print(\"Actual Hindi Sentence:\", get_hindi_sentence(y_test[i])[4:-4])\n",
        "\n",
        "    padded_input = pad_sequences([X_test[i]], maxlen=36, padding='post', truncating='post')\n",
        "\n",
        "    print(\"Predicted Hindi Translation:\", get_predicted_sentence(padded_input))  # Pass padded input directly\n",
        "    print(\"\\n\\n\\n\\n\")\n"
      ],
      "metadata": {
        "id": "vdTkSceKoB1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vQ949rpID_Qr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}